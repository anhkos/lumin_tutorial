{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9896a8cc",
   "metadata": {},
   "source": [
    "# Lesson 3: The LLM as a Translation Engine NOT COMPLETED YET\n",
    "\n",
    "Welcome to the grand finale! Let's recap what we've built so far:\n",
    "* **Lesson 1 (RAG):** We taught our LLM how to understand fuzzy human words (\"defrosted dunes\") using NASA's landform dictionary.\n",
    "* **Lesson 2 (APIs):** We wrote a Python script to send a strict Lucene query to NASA's server and get back a clickable link to view an image.\n",
    "\n",
    "Right now, *we* are still the ones doing the heavy lifting. We have to manually figure out the exact Lucene syntax and type it into our Python script. \n",
    "\n",
    "In this lesson, we are going to use the LLM to bridge that gap. We will turn the LLM into a **Translation Engine**. Its only job will be to listen to a user's natural language request and translate it into a perfectly formatted Lucene query that our API script can understand.\n",
    "\n",
    "### Guardrails: The Strict System Prompt\n",
    "By default, LLMs love to chat. If we ask it for a query, it might say: *\"Sure! Here is your query: `...` Let me know if you need anything else!\"* Our Python script can't understand that extra text. We need to use a **System Prompt** to strictly forbid the LLM from being conversational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f397528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User asked: 'Find me MRO images of Mars where the solar latitude is between 0 and 50.'\n",
      "\n",
      "ðŸ§  LLM is translating to Lucene...\n",
      "\n",
      "âœ… Generated Query: gather.common.mission:mro AND pds3_label.SOLAR_LATITUDE:[0 TO 50]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "# 1. The Strict System Prompt\n",
    "# We must explicitly tell the LLM not to chat. We ONLY want the raw query!\n",
    "LUCENE_PROMPT = \"\"\"\n",
    "You are a NASA PDS search translator. \n",
    "Your job is to translate a user's natural language request into a strict Lucene query.\n",
    "\n",
    "Use the following fields that apply to the user's query. If a field is not relevant, simply omit it from the query:\n",
    "- gather.common.mission (e.g., \"mro\", \"mars_2020\")\n",
    "- pds3_label.SOLAR_LATITUDE (e.g., [0 TO 80])\n",
    "\n",
    "CRITICAL INSTRUCTION: Return ONLY the Lucene query string. Do not include markdown formatting, quotes, or conversational text.\n",
    "\"\"\"\n",
    "\n",
    "# 2. The Translation Function\n",
    "def generate_lucene_query(user_request):\n",
    "    print(f\"User asked: '{user_request}'\\n\")\n",
    "    print(\"ðŸ§  LLM is translating to Lucene...\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"allenai/olmo-3.1-32b-instruct\", # A great model for following strict instructions\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": LUCENE_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_request}\n",
    "        ],\n",
    "        temperature=0.1 # We keep the temperature very low so it doesn't get \"creative\"\n",
    "    )\n",
    "    \n",
    "    # .strip() removes any accidental spaces or hidden newlines the LLM might have added\n",
    "    clean_query = response.choices[0].message.content.strip()\n",
    "    return clean_query\n",
    "\n",
    "# Let's test the brain!\n",
    "user_idea = \"Find me MRO images of Mars where the solar latitude is between 0 and 50.\"\n",
    "generated_query = generate_lucene_query(user_idea)\n",
    "\n",
    "print(f\"\\nâœ… Generated Query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90568e93",
   "metadata": {},
   "source": [
    "### Hooking the Brain to the Hands\n",
    "\n",
    "Look at that! The LLM successfully parsed the user's intent, remembered the exact database fields we provided in the system prompt, and output *only* the string we need. \n",
    "\n",
    "Now, we just take that generated string and plug it directly into the API Search code we wrote back in Lesson 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f66436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Sending the LLM's generated query to the NASA API...\n",
      "\n",
      "ðŸŽ‰ Success! The LLM found your image. View it here:\n",
      "https://pds-imaging.jpl.nasa.gov/tools/atlas/record?uri=atlas:pds3:mro:mars_reconnaissance_orbiter:/HiRISE/EDR/ESP/ORB_058400_058499/ESP_058410_2210/ESP_058410_2210_BG12_0.IMG\n"
     ]
    }
   ],
   "source": [
    "# 3. Plug the LLM's query directly into the NASA API\n",
    "if generated_query:\n",
    "    print(\"ðŸš€ Sending the LLM's generated query to the NASA API...\")\n",
    "    \n",
    "    search_url = \"https://pds-imaging.jpl.nasa.gov/api/search/atlas/_search\"\n",
    "    payload = {\n",
    "        \"query\": {\"query_string\": {\"query\": generated_query}},\n",
    "        \"size\": 1 \n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    api_response = requests.post(search_url, json=payload, headers=headers)\n",
    "    \n",
    "    if api_response.status_code == 200:\n",
    "        hits = api_response.json().get(\"hits\", {}).get(\"hits\", [])\n",
    "        if hits:\n",
    "            # Grab the URI of the first matching image\n",
    "            uri = hits[0].get(\"_source\", {}).get(\"uri\")\n",
    "            \n",
    "            # Format our beautiful Atlas Viewer link!\n",
    "            print(f\"\\nðŸŽ‰ Success! The LLM found your image. View it here:\")\n",
    "            print(f\"https://pds-imaging.jpl.nasa.gov/tools/atlas/record?uri={uri}\")\n",
    "        else:\n",
    "            print(\"\\nThe query was valid, but no images matched those specific filters.\")\n",
    "    else:\n",
    "        print(f\"\\nAPI Error: {api_response.status_code} - {api_response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21aacf",
   "metadata": {},
   "source": [
    "### Congratulations! \n",
    "You have successfully built an AI-powered search pipeline! \n",
    "\n",
    "By separating the \"brain\" (the LLM translating the query) from the \"hands\" (Python executing the API request), you've created a highly reliable, cost-effective way to search complex scientific databases using simple human language. \n",
    "\n",
    "**What's Next?** In advanced AI engineering, developers use protocols like **MCP (Model Context Protocol)** to allow the LLM to write the query, run the API, and read the results all on its own in a continuous loop. But the core logicâ€”translating intent into API-readable syntaxâ€”is exactly what you just mastered here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
